import gradio as gr
import json
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import os
import datetime

import cv2
import torch
import transformers
from diffusers import FlowMatchEulerDiscreteScheduler
import textwrap

from models.adapter_models import LinearAdapterWithLayerNorm
from utils.data_processor import UserInputProcessor
from utils.sd3_utils import *
from utils.utils import post_process
from utils.data_processor import UserInputProcessor
from pipelines.pipeline_sd3 import StableDiffusion3ControlNetPipeline

def check_and_process_texts(texts_str, width, height):
    try:
        if not texts_str:
            raise ValueError("texts_str cannot be None or empty")
            
        texts = json.loads(texts_str)
        
        if not texts or not isinstance(texts, list):
            raise ValueError("Invalid texts format: must be a non-empty list")
            
        if len(texts) > 7:
            raise ValueError("Too many text lines: maximum allowed is 7 lines")
            
        processed_texts = []
        
        for text in texts:
            if not isinstance(text, dict) or 'content' not in text or 'pos' not in text:
                raise ValueError("Invalid text format: each item must be a dict with 'content' and 'pos'")
                
            content = text['content']
            pos = text['pos']
            
            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
            if len(content) > 16:
                raise ValueError(f"Text too long: '{content}' exceeds 16 characters")
                
            # æ£€æŸ¥å¹¶ä¿®æ­£è¾¹ç•Œå€¼
            x1, y1, x2, y2 = pos
            x1 = max(0, min(x1, width))
            y1 = max(0, min(y1, height))
            x2 = max(0, min(x2, width))
            y2 = max(0, min(y2, height))
            
            # ç¡®ä¿ x1 < x2, y1 < y2
            x1, x2 = min(x1, x2), max(x1, x2)
            y1, y2 = min(y1, y2), max(y1, y2)
            
            processed_texts.append({
                "content": content,
                "pos": [x1, y1, x2, y2]
            })
            
        return processed_texts
        
    except json.JSONDecodeError:
        raise ValueError("Invalid JSON format in texts_str")
    except Exception as e:
        raise ValueError(f"Error processing texts: {str(e)}")


class ImageGenerator:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.data_processor = UserInputProcessor()
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œç®¡é“
        self.initialize_models()
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            self.print_memory_usage()
        
    def print_memory_usage(self):
        """è¾“å‡ºå½“å‰CUDAå†…å­˜ä½¿ç”¨çŠ¶æ€"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            print(f"CUDAå†…å­˜ä½¿ç”¨çŠ¶æ€:")
            print(f"  å·²åˆ†é…: {allocated:.2f} GB")
            print(f"  å·²é¢„ç•™: {reserved:.2f} GB") 
            print(f"  æ€»å®¹é‡: {total:.2f} GB")
            print(f"  å¯ç”¨: {total - reserved:.2f} GB\n")
        
    def initialize_models(self):
        # åŠ è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹ç»„ä»¶å’Œç®¡é“
        args = self.get_default_args()
        
        # åŠ è½½æ–‡æœ¬ç¼–ç å™¨
        text_encoder_cls_one = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision
        )
        text_encoder_cls_two = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_2"
        )
        text_encoder_cls_three = import_model_class_from_model_name_or_path(
            args.pretrained_model_name_or_path, args.revision, subfolder="text_encoder_3"
        )
        text_encoder_one, text_encoder_two, text_encoder_three = load_text_encoders(
            args, text_encoder_cls_one, text_encoder_cls_two, text_encoder_cls_three
        ) 
        # åŠ è½½åˆ†è¯å™¨
        tokenizer_one = transformers.CLIPTokenizer.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer",
            revision=args.revision,
        )
        tokenizer_two = transformers.CLIPTokenizer.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer_2",
            revision=args.revision,
        )
        tokenizer_three = transformers.T5TokenizerFast.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder="tokenizer_3",
            revision=args.revision,
        )

        # åŠ è½½VAEæ¨¡å‹
        vae = load_vae(args)
        # åŠ è½½SD3 Transformeræ¨¡å‹
        transformer = load_transfomer(args)
        # åŠ è½½è°ƒåº¦å™¨
        noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
            args.pretrained_model_name_or_path, subfolder="scheduler"
        )
        # åˆ›å»ºSceneGenNetæ§åˆ¶ç½‘ç»œ
        controlnet_inpaint = load_controlnet(args, transformer, additional_in_channel=1, num_layers=23, scratch=True)
        # åˆ›å»ºTextRenderNetæ§åˆ¶ç½‘ç»œ
        controlnet_text = load_controlnet(args, transformer, additional_in_channel=0, scratch=True)      
        # åŠ è½½é€‚é…å™¨æ¨¡å—
        adapter = LinearAdapterWithLayerNorm(128, 4096)
        
        controlnet_inpaint.load_state_dict(torch.load(args.controlnet_model_name_or_path, map_location='cpu'))
        textrender_net_state_dict = torch.load(args.controlnet_model_name_or_path2, map_location='cpu')
        controlnet_text.load_state_dict(textrender_net_state_dict['controlnet_text'])
        adapter.load_state_dict(textrender_net_state_dict['adapter'])

        # è®¾ç½®è®¾å¤‡å’Œæ•°æ®ç±»å‹
        weight_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        device = self.device

        # å°†æ‰€æœ‰æ¨¡å‹è¿ç§»è‡³ç›®æ ‡è®¾å¤‡å¹¶è®¾ç½®ç²¾åº¦
        vae = vae.to(device=device, dtype=weight_dtype)
        transformer = transformer.to(device=device, dtype=weight_dtype)
        text_encoder_one = text_encoder_one.to(device=device, dtype=weight_dtype)
        text_encoder_two = text_encoder_two.to(device=device, dtype=weight_dtype)
        text_encoder_three = text_encoder_three.to(device=device, dtype=weight_dtype)
        controlnet_inpaint = controlnet_inpaint.to(device=device, dtype=weight_dtype)
        controlnet_text = controlnet_text.to(device=device, dtype=weight_dtype)
        adapter = adapter.to(device=device, dtype=weight_dtype)

        # åŠ è½½æ¨ç†ç®¡é“
        pipeline = StableDiffusion3ControlNetPipeline(
            scheduler=FlowMatchEulerDiscreteScheduler.from_config(
                noise_scheduler.config
                ),
            vae=vae,
            transformer=transformer,
            text_encoder=text_encoder_one,
            tokenizer=tokenizer_one,
            text_encoder_2=text_encoder_two,
            tokenizer_2=tokenizer_two,
            text_encoder_3=text_encoder_three,
            tokenizer_3=tokenizer_three,
            controlnet_inpaint=controlnet_inpaint,
            controlnet_text=controlnet_text,
            adapter=adapter,
        )

        self.pipeline = pipeline.to(dtype=weight_dtype, device=device)

        # CUDA æ˜¾å­˜ä¼˜åŒ–æ¨¡å—å¼€å…³é…ç½®
        opt_config = {
            "enable_attention_slicing": True,
            "enable_vae_slicing": True,
            "enable_cpu_offload": True,
            "enable_manual_vae_slicing": True,
            "enable_gradient_checkpointing": True,
            "enable_forward_chunking": True,
            "enable_controlnet_optimization": True,
            "enable_xformers": True,
            "enable_vae_tiling": True,
        }

        # å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–æŠ€æœ¯
        if torch.cuda.is_available():
            print(f"GPUæ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            
            optimizations_enabled = []

            # æ³¨æ„åŠ›åˆ‡ç‰‡
            if opt_config["enable_attention_slicing"]:
                try:
                    if hasattr(self.pipeline, 'enable_attention_slicing'):
                        self.pipeline.enable_attention_slicing(1)
                        optimizations_enabled.append("æ³¨æ„åŠ›åˆ‡ç‰‡")
                except Exception as e:
                    print(f"æ— æ³•å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡: {e}")

            # VAEåˆ‡ç‰‡
            if opt_config["enable_vae_slicing"]:
                try:
                    if hasattr(self.pipeline, 'enable_vae_slicing'):
                        self.pipeline.enable_vae_slicing()
                        optimizations_enabled.append("VAEåˆ‡ç‰‡")
                except Exception as e:
                    print(f"æ— æ³•å¯ç”¨VAEåˆ‡ç‰‡: {e}")

            # CPU
            if opt_config["enable_cpu_offload"]:
                try:
                    if hasattr(self.pipeline, 'enable_sequential_cpu_offload'):
                        self.pipeline.enable_sequential_cpu_offload()
                        optimizations_enabled.append("åºåˆ—CPUå¸è½½")
                    elif hasattr(self.pipeline, 'enable_model_cpu_offload'):
                        self.pipeline.enable_model_cpu_offload()
                        optimizations_enabled.append("æ¨¡å‹CPUå¸è½½")
                except Exception as e:
                    print(f"æ— æ³•å¯ç”¨CPUå¸è½½: {e}")

            # æ‰‹åŠ¨VAEåˆ‡ç‰‡
            if opt_config["enable_manual_vae_slicing"]:
                try:
                    if hasattr(self.pipeline, 'vae') and hasattr(self.pipeline.vae, 'enable_slicing'):
                        self.pipeline.vae.enable_slicing()
                        optimizations_enabled.append("æ‰‹åŠ¨VAEåˆ‡ç‰‡")
                except Exception as e:
                    print(f"æ— æ³•å¯ç”¨æ‰‹åŠ¨VAEåˆ‡ç‰‡: {e}")

            # ä¸»Transformeræ¨¡å—ä¼˜åŒ–
            try:
                if hasattr(self.pipeline, 'transformer'):
                    if opt_config["enable_gradient_checkpointing"]:
                        if hasattr(self.pipeline.transformer, 'enable_gradient_checkpointing'):
                            self.pipeline.transformer.enable_gradient_checkpointing()
                            optimizations_enabled.append("æ¢¯åº¦æ£€æŸ¥ç‚¹")
                    if opt_config["enable_forward_chunking"]:
                        if hasattr(self.pipeline.transformer, 'enable_forward_chunking'):
                            self.pipeline.transformer.enable_forward_chunking(chunk_size=1)
                            optimizations_enabled.append("å‰å‘åˆ†å—")
            except Exception as e:
                print(f"æ— æ³•å¯ç”¨Transformerä¼˜åŒ–: {e}")

            # ControlNet ä¼˜åŒ–
            if opt_config["enable_controlnet_optimization"]:
                try:
                    for controlnet_name in ['controlnet_inpaint', 'controlnet_text']:
                        if hasattr(self.pipeline, controlnet_name):
                            controlnet = getattr(self.pipeline, controlnet_name)
                            if opt_config["enable_gradient_checkpointing"] and hasattr(controlnet, 'enable_gradient_checkpointing'):
                                controlnet.enable_gradient_checkpointing()
                                optimizations_enabled.append(f"{controlnet_name}æ¢¯åº¦æ£€æŸ¥ç‚¹")
                            if opt_config["enable_forward_chunking"] and hasattr(controlnet, 'enable_forward_chunking'):
                                controlnet.enable_forward_chunking(chunk_size=1)
                                optimizations_enabled.append(f"{controlnet_name}å‰å‘åˆ†å—")
                except Exception as e:
                    print(f"æ— æ³•å¯ç”¨ControlNetä¼˜åŒ–: {e}")

            # xformersæ³¨æ„åŠ›å’ŒVAEç“¦ç‰‡åŒ–
            try:
                if opt_config["enable_xformers"]:
                    if hasattr(self.pipeline.transformer, 'set_use_memory_efficient_attention_xformers'):
                        self.pipeline.transformer.set_use_memory_efficient_attention_xformers(True)
                        optimizations_enabled.append("xformersæ³¨æ„åŠ›")

                if opt_config["enable_vae_tiling"]:
                    if hasattr(self.pipeline.vae, 'enable_tiling'):
                        self.pipeline.vae.enable_tiling()
                        optimizations_enabled.append("VAEç“¦ç‰‡åŒ–")
            except Exception as e:
                print(f"æ— æ³•å¯ç”¨é¢å¤–ä¼˜åŒ–: {e}")

            # æ€»ç»“è¾“å‡º
            if optimizations_enabled:
                print(f"å·²å¯ç”¨å†…å­˜ä¼˜åŒ–æŠ€æœ¯: {', '.join(optimizations_enabled)}")
            else:
                print("æ— æ³•å¯ç”¨ä»»ä½•å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ˆè‡ªå®šä¹‰ç®¡é“æˆ–ä¸å…¼å®¹ï¼‰")

        print("")  # ç©ºè¡Œåˆ†éš”æ—¥å¿—

        # è¾“å‡ºå½“å‰æ¨¡å‹ç²¾åº¦ï¼ˆç¡®è®¤ float16 ç­‰æ˜¯å¦æ­£ç¡®è®¾ç½®ï¼‰
        print("æ¨¡å‹ç²¾åº¦é…ç½®:")
        print(f"VAEæ•°æ®ç±»å‹: {vae.dtype}")
        print(f"Transformeræ•°æ®ç±»å‹: {transformer.dtype}")
        print(f"æ–‡æœ¬ç¼–ç å™¨1æ•°æ®ç±»å‹: {text_encoder_one.dtype}")
        print(f"æ–‡æœ¬ç¼–ç å™¨2æ•°æ®ç±»å‹: {text_encoder_two.dtype}")
        print(f"æ–‡æœ¬ç¼–ç å™¨3æ•°æ®ç±»å‹: {text_encoder_three.dtype}")
        print(f"ControlNet Inpaintæ•°æ®ç±»å‹: {controlnet_inpaint.dtype}")
        print(f"ControlNet Textæ•°æ®ç±»å‹: {controlnet_text.dtype}")
        print(f"é€‚é…å™¨æ•°æ®ç±»å‹: {next(adapter.parameters()).dtype}")
        print(f"ç®¡é“æ•°æ®ç±»å‹: {self.pipeline.dtype}\n")

        
    def generate(self, main_image, mask_image, texts_str, prompt, seed_generator):
        print("å›¾åƒç”Ÿæˆå™¨å¯åŠ¨")
        try:
            print("æ¸…ç†GPUæ˜¾å­˜...")
            # æ¸…ç†æ˜¾å­˜å¹¶å¯ç”¨æœ€å¤§å†…å­˜èŠ‚çœ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()  # æ›´å½»åº•çš„å†…å­˜æ¸…ç†
            
            print("è½¬æ¢è¾“å…¥å›¾åƒæ ¼å¼...")
            # å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºnumpyæ ¼å¼ï¼ŒRGB
            main_image = np.array(main_image)
            mask = cv2.cvtColor(np.array(mask_image), cv2.COLOR_BGR2GRAY)
            print(f"ä¸»å›¾åƒå½¢çŠ¶: {main_image.shape}, æ©ç å½¢çŠ¶: {mask.shape}")
            
            print("è§£ææ–‡æœ¬å¸ƒå±€...")
            # è§£ææ–‡æœ¬å¸ƒå±€
            texts = json.loads(texts_str)
            print(f"è§£æå‡º{len(texts)}ä¸ªæ–‡æœ¬å…ƒç´ ")
            
            print("é¢„å¤„ç†è¾“å…¥æ•°æ®...")
            # é¢„å¤„ç†è¾“å…¥æ•°æ®
            input_data = self.data_processor(
                image=main_image,
                mask=mask,
                texts=texts,
                prompt=prompt
            )
            print(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œè¾“å…¥æ•°æ®é”®: {list(input_data.keys())}")
            
            # æ‰§è¡Œæ¨ç†å‰å†æ¬¡æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("æ¨ç†å‰å†…å­˜æ¸…ç†å®Œæˆ")
            
            # æ‰§è¡Œæ¨ç† - å‡å°‘æ¨ç†æ­¥æ•°ä»¥èŠ‚çœæ˜¾å­˜
            num_steps = 28 if torch.cuda.is_available() else 1  # è¿›ä¸€æ­¥å‡å°‘æ¨ç†æ­¥æ•°
            
            # ä½¿ç”¨ä¸æ•°æ®é¢„å¤„ç†ä¸€è‡´çš„åˆ†è¾¨ç‡ï¼Œé¿å…å¼ é‡å°ºå¯¸ä¸åŒ¹é…
            # data_processoré»˜è®¤ä½¿ç”¨(1024, 1024)ï¼Œæ¨ç†ä¹Ÿåº”è¯¥ä½¿ç”¨ç›¸åŒåˆ†è¾¨ç‡
            height, width = 1024, 1024  # ä¸data_processorçš„input_sizeä¿æŒä¸€è‡´
            
            print(f"ä½¿ç”¨ä¸€è‡´çš„å¤„ç†åˆ†è¾¨ç‡: {width}x{height}")
            print(f"å¼€å§‹ç®¡é“æ¨ç†ï¼Œæ­¥æ•°: {num_steps}, åˆ†è¾¨ç‡: {width}x{height}")
            
            results = self.pipeline(
                prompt=prompt,
                negative_prompt='deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, mutated hands and fingers, disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, NSFW',
                height=height,
                width=width,
                control_image=[input_data['cond_image_inpaint'], input_data['controlnet_im']],
                control_mask=input_data['control_mask'],
                text_embeds=input_data['text_embeds'],
                num_inference_steps=num_steps,
                generator=seed_generator,
                controlnet_conditioning_scale=1.0,
                guidance_scale=5.0,
                num_images_per_prompt=1,
            ).images
            
            print(f"ç®¡é“æ¨ç†å®Œæˆï¼Œç”Ÿæˆäº†{len(results)}å¼ å›¾åƒ")
            print(f"ç¬¬ä¸€å¼ å›¾åƒç±»å‹: {type(results[0])}")
            if hasattr(results[0], 'size'):
                print(f"ç¬¬ä¸€å¼ å›¾åƒå°ºå¯¸: {results[0].size}")
            
            print("å¼€å§‹åå¤„ç†...")
            # åå¤„ç†ï¼Œæ ¹æ®im_h, im_wä»relä¸­è£å‰ª[0, 0, im_w, im_h]åŒºåŸŸ
            rel = post_process(results[0], input_data['target_size'])
            print(f"åå¤„ç†å®Œæˆï¼Œæœ€ç»ˆå›¾åƒç±»å‹: {type(rel)}")
            if hasattr(rel, 'size'):
                print(f"æœ€ç»ˆå›¾åƒå°ºå¯¸: {rel.size}")
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾åƒåˆ°results/imagesç›®å½•
            try:
                # åˆ›å»ºä¿å­˜ç›®å½•
                save_dir = "results/images"
                os.makedirs(save_dir, exist_ok=True)
                
                # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"generated_{timestamp}.png"
                filepath = os.path.join(save_dir, filename)
                
                # ä¿å­˜å›¾åƒ
                if isinstance(rel, Image.Image):
                    rel.save(filepath)
                elif isinstance(rel, np.ndarray):
                    Image.fromarray(rel).save(filepath)
                
                print(f"å›¾åƒå·²ä¿å­˜è‡³: {filepath}")
            except Exception as save_e:
                print(f"ä¿å­˜å›¾åƒæ—¶å‡ºé”™: {save_e}")
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                print("æœ€ç»ˆæ˜¾å­˜æ¸…ç†å®Œæˆ")
            
            print("å›¾åƒç”ŸæˆæˆåŠŸï¼Œè¿”å›ç»“æœ")
            # è¿”å›ç”Ÿæˆçš„å›¾åƒ
            return rel
            
        except torch.cuda.OutOfMemoryError as e:
            print(f"CUDAå†…å­˜ä¸è¶³: {e}")
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
            return "CUDA out of memory. Try reducing image size or restart the application."
        except Exception as e:
            print(f"ç”Ÿæˆè¿‡ç¨‹å¼‚å¸¸: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
            # æˆªæ–­é”™è¯¯æ¶ˆæ¯ä»¥é¿å…è¿‡é•¿çš„æ–‡ä»¶å
            error_msg = str(e)
            if len(error_msg) > 100:
                error_msg = error_msg[:100] + "..."
            return f"Error in image generation: {error_msg}"
        
    def get_default_args(self):
        # è¿”å›é»˜è®¤å‚æ•°é…ç½®
        class Args:
            def __init__(self):
                self.pretrained_model_name_or_path = './checkpoints/stable-diffusion-3-medium-diffusers/'
                self.controlnet_model_name_or_path='./checkpoints/ours_weights/scenegen_net-1m-0415.pth'
                self.controlnet_model_name_or_path2='./checkpoints/ours_weights/textrender_net-1m-0415.pth'
                self.revision = None
        return Args()


def visualize_layout(main_image, mask_image, texts_str, prompt,
                     font_path: str = "./assets/fonts/AlibabaPuHuiTi-3-55-Regular.ttf",
                     margin_ratio: float = 0.92):
    """
    æ¸²æŸ“å¸¦æœ‰æ–‡å­— bbox çš„å¸ƒå±€ç¤ºæ„å›¾  
    - texts_str -> [{'pos': (x1,y1,x2,y2), 'content': "..."}] ç”±å¤–éƒ¨ `check_and_process_texts` è§£æ
    - æ–‡æœ¬å¤§å°ã€ä½ç½®ä¼šæ ¹æ®æ¡†å¤§å°è‡ªåŠ¨è°ƒæ•´å¹¶ä¿æŒå±…ä¸­
    """
    try:
        # -------- 1. è·å–ç”»å¸ƒå°ºå¯¸ -------- #
        if main_image is not None:
            height, width = main_image.shape[:2]
        elif mask_image is not None:
            height, width = mask_image.shape[:2]
        else:
            width = height = 1024

        # -------- 2. åº•å›¾ä¸ mask åˆæˆ -------- #
        canvas = Image.new("RGBA", (width, height), "white")

        if main_image is not None:
            pil_main = Image.fromarray(main_image).convert("RGBA")
        else:
            pil_main = Image.new("RGBA", (width, height), (0, 0, 0, 0))

        if mask_image is not None:
            gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)
            alpha = Image.fromarray(np.where(gray > 127, 255, 0).astype(np.uint8))
            pil_main.putalpha(alpha)

        canvas.alpha_composite(pil_main)

        # -------- 3. å·¥å…·å‡½æ•° -------- #
        def get_font(size):
            """ä¼˜é›…åœ°é™çº§åˆ°é»˜è®¤å­—ä½“"""
            try:
                return ImageFont.truetype(font_path, size)
            except OSError:
                return ImageFont.load_default()

        def optimal_font_size(text, box_w, box_h, draw):
            """äºŒåˆ†æœç´¢å¯å®¹çº³æœ€å¤§å­—å·ï¼ˆæ— éœ€ä¼  font_pathï¼Œå› ä¸º get_font è‡ªå¸¦å›é€€ï¼‰"""
            low, high, best = 1, box_h, 1
            while low <= high:
                mid = (low + high) // 2
                font = get_font(mid)
                bbox = draw.textbbox((0, 0), text, font=font)
                txt_w = bbox[2] - bbox[0]
                txt_h = bbox[3] - bbox[1]
                if txt_w <= box_w * margin_ratio and txt_h <= box_h * margin_ratio:
                    best = mid
                    low = mid + 1
                else:
                    high = mid - 1
            return best

        def wrap_text(text, font, box_w, draw):
            """æ ¹æ®å®½åº¦è‡ªåŠ¨æ¢è¡Œï¼›è¿”å› lines åˆ—è¡¨"""
            words = text.split()  # å…¼å®¹è‹±æ–‡ç©ºæ ¼ï¼Œä¸­æ–‡ç©ºæ ¼åŒç†
            if len(words) == 1:
                # çº¯ä¸­æ–‡æˆ–æ²¡æœ‰ç©ºæ ¼æ—¶ï¼šæŒ‰å­—ç¬¦ç²—æš´æˆªæ–­
                wrapped = textwrap.wrap(text, width=len(text))
            else:
                wrapped = textwrap.wrap(text, width=len(words))
            # å°è¯•ä¸æ–­æ‰©è¡Œï¼Œç›´åˆ°æ‰€æœ‰è¡Œå®½éƒ½ <= box_w*margin_ratio
            while True:
                line_too_long = False
                for i, line in enumerate(wrapped):
                    if draw.textlength(line, font=font) > box_w * margin_ratio:
                        # æŠŠè¯¥è¡Œå†æ‹†ä¸€åŠ
                        midpoint = max(1, len(line) // 2)
                        wrapped[i:i+1] = [line[:midpoint], line[midpoint:]]
                        line_too_long = True
                        break
                if not line_too_long:
                    break
            return wrapped

        draw = ImageDraw.Draw(canvas)

        # -------- 4. æ¸²æŸ“æ¯ä¸ªæ–‡æœ¬æ¡† -------- #
        texts = check_and_process_texts(texts_str, width, height)

        for item in texts:
            (x1, y1, x2, y2) = item["pos"]
            content = item["content"]

            box_w, box_h = x2 - x1, y2 - y1

            # 4.1 è®¡ç®—æœ€ä½³å­—å·
            size = optimal_font_size(content, box_w, box_h, draw)
            font = get_font(size)

            # 4.2 å¦‚æœå•è¡Œä»è¶…å®½åˆ™è‡ªåŠ¨æ¢è¡Œå¹¶è°ƒæ•´å­—å·
            lines = wrap_text(content, font, box_w, draw)
            # è‹¥æ¢è¡Œåæ€»é«˜åº¦è¶…æ¡†ï¼Œå†ç¼©å°å­—å·
            line_height = size * 1.2
            while line_height * len(lines) > box_h * margin_ratio and size > 1:
                size -= 1
                font = get_font(size)
                lines = wrap_text(content, font, box_w, draw)
                line_height = size * 1.2

            # 4.3 è®¡ç®—æ•´ä½“æ–‡æœ¬å—å°ºå¯¸
            txt_h = line_height * len(lines)
            txt_w = max(draw.textlength(line, font=font) for line in lines)

            # å·¦ä¸Šè§’åæ ‡ï¼ˆå±…ä¸­ï¼‰
            start_x = round(x1 + (box_w - txt_w) / 2)
            start_y = round(y1 + (box_h - txt_h) / 2)

            # 4.4 ç»˜åˆ¶ bbox
            draw.rectangle([x1, y1, x2, y2], outline="red", width=2)

            # 4.5 ç»˜åˆ¶æ–‡å­—
            for idx, line in enumerate(lines):
                draw.text((start_x, start_y + idx * line_height),
                          line, fill="blue", font=font, align="center")

        return canvas.convert("RGB")  # ä¸æ—§æ¥å£ä¿æŒä¸€è‡´

    except Exception as e:
        # è¿”å› Image ä»¥å¤–çš„å¯¹è±¡å¯èƒ½ä¼šç ´åè°ƒç”¨é“¾ï¼Œç›´æ¥ raise æ›´å¥½
        return RuntimeError(f"visualize_layout failed: {e}")


# ä¿®æ”¹generate_imageå‡½æ•°æ¥ä½¿ç”¨ImageGenerator
generator = ImageGenerator()

def generate_image(main_image, mask_image, texts_str, prompt, seed):
    print("å¼€å§‹å›¾åƒç”Ÿæˆæµç¨‹")
    print(f"è¾“å…¥å‚æ•° - ä¸»å›¾åƒç±»å‹: {type(main_image)}, æ©ç å›¾åƒç±»å‹: {type(mask_image)}")
    print(f"æ–‡æœ¬å­—ç¬¦ä¸²é•¿åº¦: {len(texts_str) if texts_str else 0}, æç¤ºè¯é•¿åº¦: {len(prompt) if prompt else 0}")
    
    if main_image is None:
        error_msg = "Error: Main image is required"
        print(f"é”™è¯¯: {error_msg}")
        return error_msg

    try:
        print("å¼€å§‹å¤„ç†ä¸»å›¾åƒæ ¼å¼...")
        # å¤„ç†main_imageçš„æ ¼å¼
        if isinstance(main_image, np.ndarray):
            print(f"ä¸»å›¾åƒä¸ºnumpyæ•°ç»„ï¼Œå½¢çŠ¶: {main_image.shape}")
            # å¤„ç†numpy arrayæ ¼å¼
            if main_image.ndim == 3:
                if main_image.shape[2] == 4:  # RGBAæ ¼å¼
                    print("æ£€æµ‹åˆ°RGBAæ ¼å¼")
                    rgb_array = main_image[..., :3]
                    alpha_channel = main_image[..., 3]
                    main_image = Image.fromarray(rgb_array)
                    # å¦‚æœæ²¡æœ‰æä¾›maskï¼Œä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                    if mask_image is None:
                        print("ä»alphaé€šé“åˆ›å»ºæ©ç ")
                        mask_array = (alpha_channel > 128).astype(np.uint8) * 255
                        mask_image = Image.fromarray(mask_array)
                elif main_image.shape[2] == 3:  # RGBæ ¼å¼
                    print("æ£€æµ‹åˆ°RGBæ ¼å¼")
                    main_image = Image.fromarray(main_image)
                    if mask_image is None:
                        error_msg = "Error: When using RGB image, a mask image must be provided"
                        print(f"é”™è¯¯: {error_msg}")
                        return error_msg
                else:
                    error_msg = "Error: Invalid number of channels in main image"
                    print(f"é”™è¯¯: {error_msg}")
                    return error_msg
            else:
                error_msg = "Error: Invalid dimensions for main image"
                print(f"é”™è¯¯: {error_msg}")
                return error_msg
        elif isinstance(main_image, Image.Image):
            print(f"ä¸»å›¾åƒä¸ºPIL.Imageï¼Œæ¨¡å¼: {main_image.mode}ï¼Œå°ºå¯¸: {main_image.size}")
            # å¤„ç†PIL.Imageæ ¼å¼
            if main_image.mode == 'RGBA':
                print("è½¬æ¢RGBAåˆ°RGB")
                rgb_image = main_image.convert('RGB')
                alpha_channel = main_image.split()[3]
                # å¦‚æœæ²¡æœ‰æä¾›maskï¼Œä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                if mask_image is None:
                    print("ä»alphaé€šé“åˆ›å»ºæ©ç ")
                    mask_image = alpha_channel.point(lambda x: 255 if x > 128 else 0)
                main_image = rgb_image
            elif main_image.mode != 'RGB' and mask_image is None:
                error_msg = "Error: When using RGB image, a mask image must be provided"
                print(f"é”™è¯¯: {error_msg}")
                return error_msg
        else:
            error_msg = "Error: Main image must be numpy array or PIL.Image format"
            print(f"é”™è¯¯: {error_msg}")
            return error_msg

        # ç¡®ä¿main_imageæ˜¯RGBæ¨¡å¼
        if isinstance(main_image, Image.Image) and main_image.mode != 'RGB':
            print("è½¬æ¢å›¾åƒåˆ°RGBæ¨¡å¼")
            main_image = main_image.convert('RGB')

        # å¤„ç†mask_imageçš„æ ¼å¼
        if mask_image is not None:
            print(f"å¤„ç†æ©ç å›¾åƒï¼Œç±»å‹: {type(mask_image)}")
            if isinstance(mask_image, np.ndarray):
                print(f"æ©ç å›¾åƒä¸ºnumpyæ•°ç»„ï¼Œå½¢çŠ¶: {mask_image.shape}")
                mask_image = Image.fromarray(mask_image)
            elif not isinstance(mask_image, Image.Image):
                error_msg = "Error: Mask image must be numpy array or PIL.Image format"
                print(f"é”™è¯¯: {error_msg}")
                return error_msg
            print(f"æ©ç å›¾åƒå¤„ç†å®Œæˆï¼Œå°ºå¯¸: {mask_image.size}")

        print("åˆ›å»ºéšæœºç§å­ç”Ÿæˆå™¨...")
        # ä½¿ç”¨è®¾å®šçš„seed
        seed_generator = torch.Generator(device=generator.device).manual_seed(int(seed))
        
        print("è°ƒç”¨å›¾åƒç”Ÿæˆå™¨...")
        # ä½¿ç”¨ImageGeneratorç”Ÿæˆå›¾åƒ
        generated_image = generator.generate(main_image, mask_image, texts_str, prompt, seed_generator)
        
        print(f"ç”Ÿæˆå™¨è¿”å›ç»“æœç±»å‹: {type(generated_image)}")
        
        # æ£€æŸ¥è¿”å›çš„ç»“æœç±»å‹
        if isinstance(generated_image, str):
            print(f"é”™è¯¯: ç”Ÿæˆè¿‡ç¨‹è¿”å›é”™è¯¯ä¿¡æ¯: {generated_image}")
            return generated_image
        elif isinstance(generated_image, Image.Image):
            print(f"æˆåŠŸ: æˆåŠŸç”Ÿæˆå›¾åƒï¼Œå°ºå¯¸: {generated_image.size}ï¼Œæ¨¡å¼: {generated_image.mode}")
            return generated_image
        elif isinstance(generated_image, np.ndarray):
            print(f"æˆåŠŸ: æˆåŠŸç”Ÿæˆå›¾åƒï¼Œnumpyæ•°ç»„å½¢çŠ¶: {generated_image.shape}")
            return generated_image
        else:
            error_msg = f"Error: Unexpected return type from generator: {type(generated_image)}"
            print(f"é”™è¯¯: {error_msg}")
            return error_msg
            
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        print(f"å¼‚å¸¸: ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {error_msg}")
        import traceback
        print(f"å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
        return error_msg

# For debugging
# def generate_image(main_image, mask_image, texts_str, prompt, seed):
#     try:
#         # è¿™é‡Œæ˜¯ç”Ÿæˆå›¾åƒçš„é€»è¾‘
#         # ç°åœ¨åªè¿”å›ä¸€ä¸ªå ä½å›¾åƒ
#         if main_image is None:
#             return "Error: Main image is required"
            
#         generated_image = Image.fromarray(main_image)  # ä¸´æ—¶ä½¿ç”¨è¾“å…¥å›¾åƒä½œä¸ºè¾“å‡º
#         return generated_image
    
#     except Exception as e:
#         return f"Error: {str(e)}"


# æ¸…é™¤æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„å‡½æ•°
def clear_all():
    return [None, None, "", "", 42, None, None]

# #############
# Gradioç•Œé¢éƒ¨åˆ†
with gr.Blocks() as iface:
    gr.Markdown("""
    # ğŸ¨ [CVPR2025] PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering

    ## æ–‡å­—æµ·æŠ¥å›¾åƒç”Ÿæˆ | A text poster image generation
                
    ## **ä½œè€… | Authors:** Yifan Gao\*, Zihang Lin\*, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, Hongtao Xie
                                
    <div style="display: flex; gap: 10px; justify-content: left;">
        <a href="https://github.com/eafn/PosterMaker"><img src="https://img.shields.io/badge/GitHub-Repository-blue?logo=github" alt="GitHub"></a>
        <a href="https://arxiv.org/abs/2504.06632"><img src="https://img.shields.io/badge/Paper-arXiv-red?logo=arxiv" alt="Paper"></a>
    </div>    
    """)
    gr.Markdown("""
        ---
        ## ğŸ“ æ–‡æœ¬å¸ƒå±€æ ¼å¼ | Text Layout Format
        """)

    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### æ–‡æœ¬JSONæ ¼å¼è¦æ±‚ | Text JSON Format Requirements:
            ```json
            [
                {"content": "ç¬¬ä¸€è¡Œæ–‡æœ¬", "pos": [x1, y1, x2, y2]},
                {"content": "ç¬¬äºŒè¡Œæ–‡æœ¬", "pos": [x1, y1, x2, y2]}
            ]
            ```
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### æ–‡æœ¬é™åˆ¶ | Text Limitations:
            - æœ€å¤š7è¡Œæ–‡æœ¬ | Maximum 7 lines of text
            - æ¯è¡Œâ‰¤16ä¸ªå­—ç¬¦ | â‰¤16 characters per line
            - åæ ‡ä¸è¶…è¿‡å›¾åƒè¾¹ç•Œ | Coordinates within image boundaries
            """)

    # ç¬¬ä¸€æ’ï¼šæ–‡æœ¬è¾“å…¥æ¡†å’Œseedè®¾ç½®
    with gr.Row():
        texts_input = gr.Textbox(
            label="Input JSON text layout", 
            lines=6,
            placeholder="Enter the layout JSON here...",
            scale=1,
        )
        prompt_input = gr.Textbox(
            label="Prompt", 
            lines=6,
            placeholder="Enter the generation prompt here...",
            scale=1,
        )
        seed_input = gr.Slider(
            label="Seed",
            minimum=0,
            maximum=10000,
            step=1,  # æ­¥é•¿ä¸º1ï¼Œç¡®ä¿æ˜¯æ•´æ•°
            value=42,
            scale=1,
        )
    
    gr.Markdown("""
        ---
        ## ğŸ“· å›¾åƒä¸Šä¼ è§„åˆ™ | Image Upload Rules:
        """)

    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### ä¸»å›¾åƒ(å¿…éœ€) | Subject Image (Required):
            - æ”¯æŒRGBæ ¼å¼ | Supports RGB format

            ### è’™ç‰ˆå›¾åƒ(å¿…éœ€) | Mask Image (Required):
            - RGBå›¾åƒå¿…é¡»ä¸Šä¼ é¢å¤–çš„è’™ç‰ˆå›¾åƒ | RGB image must have a separate mask image
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### è’™ç‰ˆè§„åˆ™ | Mask Rules:
            - ç™½è‰²åŒºåŸŸï¼šä¿ç•™çš„éƒ¨åˆ† | White areas: areas to keep
            - é»‘è‰²åŒºåŸŸï¼šç”Ÿæˆçš„éƒ¨åˆ† | Black areas: areas to generate
            """)

    # ç¬¬äºŒæ’ï¼šå›¾åƒè¾“å…¥
    with gr.Row():
        with gr.Column(scale=1):
            main_image_input = gr.Image(
                label="Upload Subject Image", 
                height=400,
            )
        with gr.Column(scale=1):
            mask_image_input = gr.Image(
                label="Upload Mask Image", 
                height=400,
            )
    
    # æé†’ä¿¡æ¯
    gr.Markdown("""
        ---
        ## âš ï¸ é‡è¦æç¤º | Important Notes:
        """)
    with gr.Row():
        with gr.Column():
            gr.Markdown("""
            ### é¢„è§ˆæ­¥éª¤ | Preview Steps:
            - è¯·å…ˆä½¿ç”¨"Visualize Layout"æŒ‰é’®é¢„è§ˆæ–‡æœ¬å¸ƒå±€ | Please use "Visualize Layout" button first to preview text layout
            - ç¡®è®¤å¸ƒå±€æ— è¯¯åå†ç‚¹å‡»"Generate Image"ç”Ÿæˆå›¾åƒ | Click "Generate Image" after confirming the layout is correct
            """)
        
        with gr.Column():
            gr.Markdown("""
            ### ç­‰å¾…è¯´æ˜ | Wait Time:
            - å›¾åƒç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾… | Image generation may take some time, please be patient
            """)
        
    # ç¬¬ä¸‰æ’ï¼šæŒ‰é’®
    with gr.Row():
        visualize_btn = gr.Button("Visualize Layout")
        generate_btn = gr.Button("Generate Image")
        clear_btn = gr.Button("Clear All")
    
    # ç¬¬å››æ’ï¼šè¾“å‡ºå›¾åƒ
    with gr.Row():
        with gr.Column(scale=1):
            layout_output = gr.Image(
                label="Layout Visualization", 
                height=400,
            )
        with gr.Column(scale=1):
            generated_output = gr.Image(
                label="Generated Image", 
                height=400,
            )

    gr.Markdown("""
        ---
        ## ç¤ºä¾‹ | Examples:
        """)
    # è®¾ç½®ç¤ºä¾‹
    examples = [
        [
            json.dumps([
                {"content": "æŠ¤è‚¤ç¾é¢œè´µå¦‡ä¹³", "pos": [69, 104, 681, 185]},
                {"content": "99.9%çº¯åº¦ç»è‰²å› ", "pos": [165, 226, 585, 272]},
                {"content": "æŒä¹…ä¿å¹´è½»", "pos": [266, 302, 483, 347]}
            ], ensure_ascii=False),
            "The subject rests on a smooth, dark wooden table, surrounded by a few scattered leaves and delicate flowers,with a serene garden scene complete with blooming flowers and lush greenery in the background.",
            './images/rgba_images/571507774301.png',
            './images/subject_masks/571507774301.png',
            42
        ],
        [
            json.dumps([
                {"content": "å¢å¼ºå…ç–«åŠ›", "pos": [38, 38, 471, 127]},
                {"content": "å¹¼å„¿å¥¶ç²‰", "pos": [38, 143, 356, 224]},
                {"content": "æ˜“äºå†²è°ƒ", "pos": [67, 259, 219, 296]}
            ], ensure_ascii=False),
            "The golden can of milk powder rests on a smooth, dark wooden table, surrounded by a few scattered leaves and delicate flowers, with a serene garden scene complete with blooming flowers and lush greenery in the background.",
            './images/rgba_images/652158680541.png',
            './images/subject_masks/652158680541.png',
            42
        ],
        [
            json.dumps([
                {"content": "CABæ’ä¹…æ°”å«", "pos": [85, 101, 720, 192]},
                {"content": "æŒä¹…ä¸è„±å¦†", "pos": [294, 226, 511, 271]}
            ], ensure_ascii=False),
            "A subject sits elegantly on smooth, light beige fabric, surrounded by a backdrop of similarly draped material that offers a silky appearance. To the left, a delicate white flower injects a subtle natural element into the composition. The overall environment is clean, bright, and minimalistic, exuding a sense of sophistication and simplicity that highlights the subject beautifully.",
            './images/rgba_images/809702153676.png',
            './images/subject_masks/809702153676.png',
            888
        ],
            [
        json.dumps([
            {"content": "åŸåˆ›æ–°æ¬¾", "pos": [135, 60, 686, 199]},
            {"content": "å¡é€šæ¬¾æ‰‹æœºå£³", "pos": [246, 236, 575, 299]}
        ], ensure_ascii=False),
        "The poster features a vibrant yellow background adorned with playful cartoons, including rainbows, clouds, and stars. Characters perform activities like carrying bags and holding hearts, adding a dynamic feel. Comic-style text amplifies the cheerful vibe. The solid yellow backdrop ensures the product stands out. The poster uses eye-catching fonts and text, offering clear visuals that blend harmonious design with sophistication.",
        './images/rgba_images/749870344644.png',
        './images/subject_masks/749870344644.png',
        1000
        ]
    ]
    gr.Examples(
        examples=examples,
        inputs=[texts_input, prompt_input, main_image_input, mask_image_input, seed_input]
    )

    # åŒ…è£…generate_imageå‡½æ•°ï¼Œæ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
    def generate_image_with_logging(main_image, mask_image, texts_str, prompt, seed):
        print("\n" + "="*50)
        print("å›¾åƒç”Ÿæˆæµç¨‹å¯åŠ¨")
        print("="*50)
        
        result = generate_image(main_image, mask_image, texts_str, prompt, seed)
        
        print(f"ç”Ÿæˆç»“æœç±»å‹: {type(result)}")
        if isinstance(result, str):
            print(f"æœ€ç»ˆé”™è¯¯: {result}")
        else:
            print("å›¾åƒç”Ÿæˆæµç¨‹å®Œæˆ")
        
        print("="*50)
        print("å›¾åƒç”Ÿæˆæµç¨‹ç»“æŸ")
        print("="*50 + "\n")
        
        return result

    # è®¾ç½®æŒ‰é’®äº‹ä»¶
    visualize_btn.click(
        fn=visualize_layout,
        inputs=[main_image_input, mask_image_input, texts_input, prompt_input],
        outputs=layout_output
    )
    
    generate_btn.click(
        fn=generate_image_with_logging,
        inputs=[main_image_input, mask_image_input, texts_input, prompt_input, seed_input],
        outputs=generated_output
    )
    
    # æ¸…é™¤æŒ‰é’®äº‹ä»¶
    clear_btn.click(
        fn=clear_all,
        inputs=[],
        outputs=[main_image_input, mask_image_input, texts_input, prompt_input, 
                seed_input, layout_output, generated_output]
    )


# main function
if __name__ == "__main__":
    iface.launch(server_name="0.0.0.0",server_port=7861)

